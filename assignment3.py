# -*- coding: utf-8 -*-
"""Assignment3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pMq7ipI7pIOSxEAH-UbdvTeeYgtjzOYG
"""

# Install Hugging Face libraries into the Colab environment
!pip install transformers datasets huggingface_hub --quiet
!pip install transformers datasets tokenizers torch
!pip install fsspec==2023.9.0

import transformers
print(transformers.__version__)
!pip install -U "fsspec==2024.3.1" "gcsfs==2024.3.1" datasets --quiet
!pip install "datasets==2.17.1" --quiet #install compatible versions

from datasets import load_dataset
dataset = load_dataset("imdb") #load dataset
print(dataset["train"][0])

from transformers import AutoModelForSequenceClassification, AutoTokenizer
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

def tokenize(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)
tokenized_dataset = dataset.map(tokenize, batched=True) #tokenizing the dataset

from transformers import Trainer, TrainingArguments
training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    num_train_epochs=3,
    fp16=True,
)

from sklearn.metrics import accuracy_score, f1_score
import numpy as np

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=1)
    return {
        "accuracy": accuracy_score(labels, predictions),
        "f1": f1_score(labels, predictions, average="weighted")
    }

trainer = Trainer(model=model,
                  args=training_args,
                  train_dataset=tokenized_dataset["train"].select(range(20000)),
                  eval_dataset=tokenized_dataset["test"].select(range(1000)),
                  compute_metrics=compute_metrics
                  )
trainer.train()

results = trainer.evaluate()
print("Accuracy:", results["eval_accuracy"])
print("F1 Score:", results["eval_f1"]) #printing accuracy and f1 metrics

model.save_pretrained("./fine_tuned_model")
tokenizer.save_pretrained("./fine_tuned_model") #saving the model and the tokenizer

from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

# Load model and tokenizer from saved directory
model_path = "./fine_tuned_model"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)

# Create a pipeline for sentiment analysis
classifier = pipeline("text-classification", model=model, tokenizer=tokenizer)

# Run inference
sample_text = "It was the worst experience I have ever had!"
result = classifier(sample_text)

label_map = {"LABEL_0": "negative", "LABEL_1": "positive"}
predicted_label = label_map.get(result[0]["label"], result[0]["label"])

print(f"Predicted Sentiment: {predicted_label}")